{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries and set up default plot params\n",
    "\n",
    "#### Note, this cell picks the path from which you want to load tha data and to which you want to save all figures as your current working directory (`cwd`).\n",
    "#### If you want to load from/save to a different path, edit the `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import scipy.stats\n",
    "from scipy.stats import binned_statistic_2d\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, PowerTransformer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import brier_score_loss, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set default tick label size\n",
    "matplotlib.rcParams.update({'xtick.labelsize': 16})\n",
    "matplotlib.rcParams.update({'ytick.labelsize': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv file\n",
    "df = pd.read_csv(path + '/' + 'haberman.data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of patients in each state, where state 1 means that the patient survived 5 years or longer, and state 2 tells us that the patient died within 5 years\n",
    "\n",
    "Also, save the fraction of the dataset made up by the positive class to use later, where (for an imbalanced dataset) the smallest class represents the positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each state\n",
    "states = df['STATE'].values\n",
    "count = Counter(states)\n",
    "min_v = len(df)+1\n",
    "for k,v in count.items():\n",
    "    p = v / len(states) * 100.0\n",
    "    print('State={0:d}, Count={1:d}, Frac={2:.1f}%'.format(k, v, p))\n",
    "    # Save the fraction of the dataset made up by the positive class\n",
    "    if v < min_v:\n",
    "        per_pos = p/100.0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide the columns into inputs (age, year, nodes) and outputs (state), and encode the output/target variable (y) to have values 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "\n",
    "# label encode the target variable to have the classes 0 and 1\n",
    "y = LabelEncoder().fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for evaluating the skill of the model\n",
    "\n",
    "#### Use the Brier score, which calculates the mean squared error between the model predicted probabilities and the probabilities expected from the reference dataset\n",
    "We calculate the reference dataset Brier score, where per_pos represents the expected baseline performance for the predictive model, and the model Brier score\n",
    "\n",
    "We then calculate the model skill score, by comparing the Brier score for the reference and the model\n",
    "By default, a skill score of 0.0 is a perfect score, but we invert such that 1.0 is a perfect score, and a score of 0.0 means the model performs exactly as well as the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Brier skill score (BSS)\n",
    "# Use as a metric for evaluating the skill of the model based on the returned predicted probabilities\n",
    "def brier_skill_score(y_true, y_prob):\n",
    "    # Calculate Brier score for the reference (i.e., the dataset)\n",
    "    ref_probs = [per_pos for _ in range(len(y_true))]\n",
    "    bs_ref = brier_score_loss(y_true, ref_probs)\n",
    "    # Calculate Brier score for the predictive model\n",
    "    bs_model = brier_score_loss(y_true, y_prob)\n",
    "    # Calculate skill score, by comparing the Brier score for the reference and the model\n",
    "    print(1.0 - (bs_model / bs_ref))\n",
    "    return 1.0 - (bs_model / bs_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use cross-validation, which uses a limited sample of a dataset in order to estimate the skill of a model, or how the model is expected to perform when used to make predictions on data not used in the training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(X, y, model):\n",
    "    # k-fold cross-validation\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # Tell the scorer what metric to use\n",
    "    metric = make_scorer(brier_skill_score, needs_proba=True)\n",
    "    # Evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As a check, we want to evaluate the baseline strategy of predicting the distribution of positive examples in the training dataset\n",
    "We set the “strategy” to “prior” so the model will predict the prior probability of each class in the training dataset, which for the positive class we know is equal to per_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reference model\n",
    "model = DummyClassifier(strategy='prior')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We then evaluate the baseline model, which we expect to have a BSS of 0.0, i.e., the same as the reference model, because it IS the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "scores = evaluate_model(X, y, model)\n",
    "# Summarize performance\n",
    "print('Mean BSS: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate test models\n",
    "\n",
    "#### We will use the functions defined previously to evaluate a set of models that are known to be effective at predicting probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = [LogisticRegression(solver='lbfgs'), LinearDiscriminantAnalysis(),\n",
    "          QuadraticDiscriminantAnalysis(), GaussianNB(), MultinomialNB(),\n",
    "          GaussianProcessClassifier()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will compare the results from each model based on the mean Brier skill scoreand the distribution of the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, values = list(), list()\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    # Get a name for the model, used for plotting\n",
    "    name = type(model).__name__[:10]\n",
    "    # Evaluate the model and save results\n",
    "    scores = evaluate_model(X, y, model)\n",
    "    # Summarize performance\n",
    "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "    names.append(name)\n",
    "    values.append(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a box and whisker plot that shows the distribution of results from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.boxplot(values, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate test models with scaled inputs\n",
    "\n",
    "#### It is good practice to scale data if the variables have different units of measure, as they do in this case\n",
    "\n",
    "First, we use the StandardScaler, which is fit to the training dataset and applied within each k-fold cross-validation evaluation. Because scaling will shift the data to a mean of zero and unit standard deviation, we drop the MultinomialNB() model as it does not support negative input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine models, drop the MultinomialNB() model\n",
    "models = [LogisticRegression(solver='lbfgs'), LinearDiscriminantAnalysis(),\n",
    "          QuadraticDiscriminantAnalysis(), GaussianNB(), GaussianProcessClassifier()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, compare the results from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, values = list(), list()\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    # Get a name for the model, used for plotting\n",
    "    name = type(model).__name__[:10]\n",
    "    # Create a pipeline\n",
    "    pip = Pipeline(steps=[('t', StandardScaler()),('m',model)])\n",
    "    # Evaluate the model and save results\n",
    "    scores = evaluate_model(X, y, pip)\n",
    "    # Summarize performance\n",
    "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "    names.append(name)\n",
    "    values.append(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And create a box and whisker plot that shows the distribution of results from each model\n",
    "\n",
    "The top three performing models show a similar spread in scores, suggesting that these models give the same general mapping of inputs to probabilities. We drop the QuadraticDiscriminantAnalysis() and GaussianNB() models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.boxplot(values, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, it is good practice to scale data if the variables have different units of measure, as they do in this case\n",
    "\n",
    "Next, we use the PowerTransfor, which automatically determines how to make each variable more Gaussian. Because the power transform uses a log function, we also have to scale the dataset using a MinMaxScaler so that none of the values are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(solver='lbfgs'), LinearDiscriminantAnalysis(), GaussianProcessClassifier()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, values = list(), list()\n",
    "# Evaluate each model\n",
    "for model in models:\n",
    "    # Get a name for the model, used for plotting\n",
    "    name = type(model).__name__[:10]\n",
    "    # Create a pipeline\n",
    "    pip = Pipeline(steps=[('t1', MinMaxScaler()), ('t2', PowerTransformer()),('m',model)])\n",
    "    # Evaluate the model and save results\n",
    "    scores = evaluate_model(X, y, pip)\n",
    "    # Summarize performance\n",
    "    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n",
    "    names.append(name)\n",
    "    values.append(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And create a box and whisker plot that shows the distribution of results from each model\n",
    "\n",
    "The box and whisker plot suggests a smaller and more focused spread for LR compared to the LDA, which was the second-best performing method.\n",
    "\n",
    "All methods still show skill on average, though the distribution of scores show runs that drop below 0.0 (no skill) in some cases.\n",
    "\n",
    "#### We choose the LogisticRegression() model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.boxplot(values, labels=names, showmeans=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline and fit the model\n",
    "model = Pipeline(steps=[('t1', MinMaxScaler()), ('t2', PowerTransformer()),('m',LogisticRegression(solver='lbfgs'))])\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run some test cases\n",
    "\n",
    "#### Here, we'll check a few patients, pulled directly from the dataset, that we know survived (state 1) or did not survive (state 2) for longer than 5 years after their surgery. The model will output the probability of survival for each case.\n",
    "\n",
    "In cases where we know that the patient selected did live for 5 years or longer after the surgery (state 1), the model predicts a probability of survival > 80%, where the predicted probability of survival for patients who did not live for 5 years after the surgery (state 2) was < 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival (state 1) test cases\n",
    "print('Known Survival Cases:')\n",
    "data = [[31,59,2], [66,58,0], [34,60,1]]\n",
    "for row in data:\n",
    "    # Make model prediction\n",
    "    yhat = model.predict_proba([row])\n",
    "    # Probability of survival\n",
    "    p_survive = yhat[0, 0] * 100\n",
    "    # Summarize\n",
    "    print('>data=%s, Survival=%.3f%%' % (row, p_survive))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-survival (state 2) test cases\n",
    "print('Known Non-Survival Cases:')\n",
    "data = [[44,64,6], [34,66,9], [38,69,21]]\n",
    "for row in data:\n",
    "    # Make model prediction\n",
    "    yhat = model.predict_proba([row])\n",
    "    ## Probability of survival\n",
    "    p_survive = yhat[0, 0] * 100\n",
    "    # Summarize\n",
    "    print('data=%s, Survival=%.3f%%' % (row, p_survive))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
